{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a01eade",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from __future__ import absolute_import, print_function\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "from test_config import cfg\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e852ea5e",
   "metadata": {},
   "source": [
    "# Load model, output stage metadata whilst predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a44cb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate root folder\n",
    "path_lst = os.getcwd().split(os.sep)[:4]\n",
    "path_lst[0] += os.sep\n",
    "root_folder = os.path.join(*path_lst)\n",
    "\n",
    "# C3 and experiment folders\n",
    "c3_folder = os.path.join(root_folder, 'C-3-Framework')\n",
    "exp_folder = os.path.join(c3_folder, 'exp')\n",
    "\n",
    "# Append to sys.path\n",
    "sys.path.append(c3_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53b83faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_classes import CvTest\n",
    "from model_files.VGG16_LCM_REG import VGG16_LCM_REG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cc10ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-02-16 09:10:02] === Experiment:202202080823_data=london_model=VGG16_LCM_REG_lr=1e-05 ===\n",
      "[2022-02-16 09:10:02] model: VGG16_LCM_REG\n",
      "[2022-02-16 09:10:02] CC: LCM\n",
      "[2022-02-16 09:10:02] ===PAPER REVIEW VERSION===\n"
     ]
    }
   ],
   "source": [
    "# Initialise\n",
    "exp = '202202080823_data=london_model=VGG16_LCM_REG_lr=1e-05'\n",
    "\n",
    "test = CvTest(\n",
    "    exp=exp,\n",
    "    net_object=VGG16_LCM_REG\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c095b042",
   "metadata": {},
   "source": [
    "## Dissect logs, output arrays to .npy files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b205a7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-02-16 09:05:48] x3:torch.Size([1, 512, 128, 128])\n",
      "[2022-02-16 09:05:48] x4:torch.Size([1, 512, 64, 64])\n",
      "[2022-02-16 09:05:48] x5:torch.Size([1, 512, 32, 32])\n",
      "[2022-02-16 09:05:48] multifuse x5:torch.Size([1, 512, 32, 32])\n",
      "[2022-02-16 09:05:48] multifuse x4:torch.Size([1, 512, 64, 64])\n",
      "[2022-02-16 09:05:48] multifuse x3:torch.Size([1, 512, 128, 128])\n",
      "[2022-02-16 09:05:48] count_layer x5_:torch.Size([1, 512, 16, 16])\n",
      "[2022-02-16 09:05:48] p_layer x5:torch.Size([1, 3, 16, 16])\n",
      "[2022-02-16 09:05:48] k_layer x5:torch.Size([1, 1, 16, 16])\n",
      "[2022-02-16 09:05:48] i_layer x5:torch.Size([1, 3, 16, 16])\n",
      "[2022-02-16 09:05:48] stage1_regress0: torch.Size([1, 16, 16])\n",
      "[2022-02-16 09:05:48] stage1_regress1_0: torch.Size([1, 16, 16])\n",
      "[2022-02-16 09:05:48] stage1_regress1_1: torch.Size([1, 16, 16])\n",
      "[2022-02-16 09:05:48] stage1_regress1_2: torch.Size([1, 16, 16])\n",
      "[2022-02-16 09:05:48] stage1_regress2: torch.Size([1, 1, 16, 16])\n",
      "[2022-02-16 09:05:48] stage1_regress3: torch.Size([1, 1, 16, 16])\n",
      "[2022-02-16 09:05:48] [i:1][filename:00_02_51.56213176_-0.16479492_19_1024_1024.jpg][mae: 49.6][gt: 108.3][pred: 157.9]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[img_raw: (1024, 1024)]\n",
       "[img: torch.Size([1, 3, 1024, 1024])]\n",
       "[gt_raw: (1024, 1024)]\n",
       "[gt: (1, 16, 16)]\n",
       "[pred_map: (16, 16)]\n",
       "[pred_map_resize: (128, 128)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate prediction\n",
    "test.pred(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ecf2b0",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42eadea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mode = cfg.DATASET\n",
    "if data_mode == 'SHHA':\n",
    "    patch_max = cfg.SHHAPATCHMAX\n",
    "elif data_mode == 'SHHB':\n",
    "    patch_max = cfg.SHHBPATCHMAX\n",
    "elif data_mode == 'QNRF':\n",
    "    patch_max = cfg.QNRFPATCHMAX\n",
    "elif data_mode == 'UCF50':\n",
    "    patch_max = cfg.CC50PATCHMAX\n",
    "    \n",
    "elif data_mode == 'london':\n",
    "    patch_max = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a8ef885",
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_num = [3,3,3]\n",
    "lambda_i = 1\n",
    "lambda_k = 1\n",
    "count_range = patch_max\n",
    "multi_fuse = True\n",
    "soft_interval = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ed2ee3",
   "metadata": {},
   "source": [
    "\n",
    "### Layer 3\n",
    "\n",
    "in:1024x1024x3\n",
    "\n",
    "out: 128x128x512\n",
    "\n",
    "### Layer 4\n",
    "\n",
    "in: 128x128x512\n",
    "\n",
    "out: 64x64x512\n",
    "\n",
    "### Layer 5\n",
    "\n",
    "in: 64x64x512\n",
    "\n",
    "out: 32x32x512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6dfe5ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def VGG_make_layers(cfg, in_channels=3, batch_norm=False, dilation=1):\n",
    "    d_rate = dilation\n",
    "    layers = []\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=d_rate, dilation=d_rate)\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "layer3 = VGG_make_layers([64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512])\n",
    "layer4 = VGG_make_layers(['M', 512, 512, 512], in_channels=512)\n",
    "layer5 = VGG_make_layers(['M', 512, 512, 512], in_channels=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b57956c",
   "metadata": {},
   "source": [
    "## Scale aware module\n",
    "\n",
    "Enhance the multi-scale feature extraction capability of the network. Achieves multi-scale\n",
    "information enhancement only on a single layer feature map and performs this\n",
    "operation at different convolutional layers to bring rich information to subsequent regression modules.\n",
    "\n",
    "SAM first compresses the channel of feature map via 1×1 convolution. Afterwards, the compressed\n",
    "feature map is processed through dilated convolution with different expansion\n",
    "ratios of 1, 2, 3 and 4 to perceive multi-scale features. The extracted\n",
    "multi-scale feature maps are fused via channel-wise concatenation operation and\n",
    "3×3 convolution. The size of final feature map is consistent with the input one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "364fbe65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DC_layer(nn.Module):\n",
    "    \"\"\"\n",
    "    Direct counting layer\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, level, fuse=False):\n",
    "        super(DC_layer, self).__init__()\n",
    "        self.level = level\n",
    "        self.conv1x1_d1 = nn.Conv2d(512, 512, kernel_size=1)\n",
    "        self.conv1x1_d2 = nn.Conv2d(512, 512, kernel_size=1)\n",
    "        self.conv1x1_d3 = nn.Conv2d(512, 512, kernel_size=1)\n",
    "        self.conv1x1_d4 = nn.Conv2d(512, 512, kernel_size=1)\n",
    "\n",
    "        self.conv_d1 = nn.Conv2d(512, 512, kernel_size=3, padding=1, dilation=1)\n",
    "        self.conv_d2 = nn.Conv2d(512, 512, kernel_size=3, padding=2, dilation=2)\n",
    "        self.conv_d3 = nn.Conv2d(512, 512, kernel_size=3, padding=3, dilation=3)\n",
    "        self.conv_d4 = nn.Conv2d(512, 512, kernel_size=3, padding=4, dilation=4)\n",
    "        \n",
    "        self.fuse = fuse\n",
    "        if self.fuse:\n",
    "            self.fuse = nn.Conv2d(512*2, 512, kernel_size=3, padding=1)\n",
    "            self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.conv1x1_d1(x)\n",
    "        x2 = self.conv1x1_d2(x)\n",
    "        x3 = self.conv1x1_d3(x)\n",
    "        x4 = self.conv1x1_d4(x)\n",
    "\n",
    "        x1 = self.conv_d1(x1)\n",
    "        x2 = self.conv_d2(x2)\n",
    "        x3 = self.conv_d3(x3)\n",
    "        x4 = self.conv_d4(x4)\n",
    "\n",
    "        # x = torch.cat([x1, x2, x3, x4], dim=1)\n",
    "        # x = self.relu(self.fuse(x))\n",
    "        x = Maxout(x1, x2, x3, x4)\n",
    "        return x\n",
    "    \n",
    "def Maxout(x1, x2, x3, x4):\n",
    "\n",
    "    # mask for x1 >= x2\n",
    "    mask_1 = torch.ge(x1, x2)\n",
    "    mask_1 = mask_1.float()\n",
    "    x = mask_1 * x1 + (1-mask_1) * x2\n",
    "    \n",
    "    print(x)\n",
    "\n",
    "    mask_2 = torch.ge(x, x3)\n",
    "    mask_2 = mask_2.float()\n",
    "    x = mask_2 * x + (1-mask_2) * x3\n",
    "    \n",
    "    print(x)\n",
    "\n",
    "    mask_3 = torch.ge(x, x4)\n",
    "    mask_3 = mask_3.float()\n",
    "    x = mask_3 * x + (1-mask_3) * x4\n",
    "    return x\n",
    "\n",
    "fuse_layer5 = DC_layer(level=0)\n",
    "fuse_layer4 = DC_layer(level=1)\n",
    "fuse_layer3 = DC_layer(level=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1d0f0c",
   "metadata": {},
   "source": [
    "### Exploring Maxout function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "289b4661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 1.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 2.],\n",
      "        [0., 0., 0.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 2., 2.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = torch.Tensor([\n",
    "    [1,0,0],\n",
    "    [0,1,0],\n",
    "    [0,0,0]\n",
    "])\n",
    "\n",
    "x2 = torch.Tensor([\n",
    "    [0,0,0],\n",
    "    [0,1,1],\n",
    "    [0,0,0]\n",
    "])\n",
    "\n",
    "x3 = torch.Tensor([\n",
    "    [0,0,0],\n",
    "    [0,1,2],\n",
    "    [0,0,0]\n",
    "])\n",
    "\n",
    "x4 = torch.Tensor([\n",
    "    [0,0,0],\n",
    "    [0,2,2],\n",
    "    [0,0,0]\n",
    "])\n",
    "\n",
    "Maxout(x1, x2, x3, x4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f95eef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_1 = torch.ge(x1, x2)\n",
    "mask_1 = mask_1.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "65ba732e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1., 1.],\n",
       "         [1., 1., 0.],\n",
       "         [1., 1., 1.]]),\n",
       " tensor([[0., 0., 0.],\n",
       "         [0., 0., 1.],\n",
       "         [0., 0., 0.]]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_1, 1-mask_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700b4eae",
   "metadata": {},
   "source": [
    "## Notes on 1x1 convolution\n",
    "\n",
    "[Machine learning mastery link](https://machinelearningmastery.com/introduction-to-1x1-convolutions-to-reduce-the-complexity-of-convolutional-neural-networks/)\n",
    "\n",
    "Channel-wise pooling/feature map pooling/projection layer\n",
    "\n",
    "Similar to how pooling downscales feature maps by halving the width and height of the feature maps, 1x1 conv layer reduces the depth or number of feature maps.\n",
    "\n",
    "This will only have a single parameter for each channel, and will result in a single output value. This acts like a single neuron with input from same position in each feature map. Not really a convolution operation, linear weighting or projection of the input.\n",
    "\n",
    "[Visualisation from deeplearning.ai](https://www.coursera.org/lecture/convolutional-neural-networks/networks-in-networks-and-1x1-convolutions-ZTb8x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7030afbf",
   "metadata": {},
   "source": [
    "## Count layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e8bf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Count_layer(nn.Module):\n",
    "    \"\"\"\n",
    "    Avg pool, max pool, concat\n",
    "    1x1 conv/merge concatenated layer\n",
    "    \"\"\"\n",
    "    def __init__(self, inplanes=512, pool=2):\n",
    "        super(Count_layer, self).__init__()\n",
    "        self.avgpool_layer = nn.Sequential(\n",
    "            nn.Conv2d(inplanes, inplanes, kernel_size=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AvgPool2d((pool, pool), stride=pool),\n",
    "        )\n",
    "        self.maxpool_layer = nn.Sequential(\n",
    "            nn.Conv2d(inplanes, inplanes, kernel_size=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d((pool, pool), stride=pool),\n",
    "        )\n",
    "        self.conv1x1= nn.Sequential(\n",
    "            nn.Conv2d(inplanes*2, inplanes, kernel_size=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_avg = self.avgpool_layer(x)\n",
    "        x_max = self.maxpool_layer(x)\n",
    "\n",
    "        x = torch.cat([x_avg, x_max], dim=1)\n",
    "        x = self.conv1x1(x)\n",
    "        return x\n",
    "    \n",
    "count_layer5 = Count_layer(pool=2)\n",
    "count_layer4 = Count_layer(pool=4)\n",
    "count_layer3 = Count_layer(pool=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0994a0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer5_k = nn.Sequential(\n",
    "    nn.Conv2d(512, 1, kernel_size=1),\n",
    "    nn.Tanh(),\n",
    ")\n",
    "layer5_i = nn.Sequential(\n",
    "    nn.Conv2d(512, self.stage_num[0], kernel_size=1),\n",
    "    nn.Sigmoid(),\n",
    ")\n",
    "layer5_p = nn.Sequential(\n",
    "    nn.Conv2d(512, self.stage_num[0], kernel_size=1),\n",
    "    nn.ReLU(),\n",
    ")\n",
    "\n",
    "layer4_k = nn.Sequential(\n",
    "    nn.Conv2d(512, 1, kernel_size=1),\n",
    "    nn.Tanh(),\n",
    ")\n",
    "layer4_i = nn.Sequential(\n",
    "    nn.Conv2d(512, self.stage_num[0], kernel_size=1),\n",
    "    nn.Sigmoid(),\n",
    ")\n",
    "layer4_p = nn.Sequential(\n",
    "    nn.Conv2d(512, self.stage_num[0], kernel_size=1),\n",
    "    nn.ReLU(),\n",
    ")\n",
    "\n",
    "layer3_k = nn.Sequential(\n",
    "    nn.Conv2d(512, 1, kernel_size=1),\n",
    "    nn.Tanh(),\n",
    ")\n",
    "layer3_i = nn.Sequential(\n",
    "    nn.Conv2d(512, self.stage_num[0], kernel_size=1),\n",
    "    nn.Sigmoid(),\n",
    ")\n",
    "layer3_p = nn.Sequential(\n",
    "    nn.Conv2d(512, self.stage_num[0], kernel_size=1),\n",
    "    nn.ReLU(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83eb2013",
   "metadata": {},
   "source": [
    "## Activation functions\n",
    "\n",
    "#### *Deep Learning for Vision systems, Mohamed Elgendy, pp51-61*\n",
    "\n",
    "\"The purpose of an activation function is to introduce non-linearity into the network. Without it it will perform similary to a single perceptron no matter how many layers we add. \n",
    "\n",
    "They are needed to restrict the output value to a finite value.\n",
    "\n",
    "The composition of 2 linear functions is a linear function so unless you use a non-linear activation function you are not computing any interesting functions no matter how deep you make the network.\n",
    "\n",
    "When we derrive a linear function we get a constant so it doesn't depend on the input value. This means every time we do backprop the gradient will be the same. This means we're not improving the error\n",
    "\n",
    "### Heaviside\n",
    "Produces a binary output, mainly used in binary classification problems to predict class\n",
    "\n",
    "### Sigmoid/logistic function\n",
    "\n",
    "$ \\frac{1}{1 + e^{-z}} $\n",
    "\n",
    "Often used in binary classification problems to predict the probability of a class when you have 2 classes. Converts infinite continuous variables into simple probabilities between 0 and 1. \n",
    "\n",
    "In example where linear models produce probabilities greater than 1 or less than zero. Using exponents gets rid of the values less than 0, and dividing an equation by itself plus a small value will give us a number smaller than 1.\n",
    "\n",
    "### Softmax function\n",
    "\n",
    "$ \\sigma_j = \\frac{e^{x_j}}{\\sum{e^{x_i}}} $\n",
    "\n",
    "Generalisation of the sigmoid function. Used to obtain classification probabilities when we have more than two classes. Forces the outputs of a network to sum to 1. This also works fine when classifying between 2 classes\n",
    "\n",
    "### Hyperbolic tangent (tanh)\n",
    "Shifted version of the sigmoid. Instead of between 0 and 1, between -1 and 1. Almost always works better than sigmoids in hidden layers because it has the effect of centering your data so that the mean of the data is close to zero rather than 0.5, which makes the learning for the next layer a bit easier. \n",
    "\n",
    "One downside to tanh and sigmoid is if the value if very large the gradient is almost zero\n",
    "\n",
    "### ReLU\n",
    "Piecewise linear function that is zero for all values less than or equal to zero, and a*x for all values greater than zero\n",
    "\n",
    "[Why relu works](https://towardsdatascience.com/if-rectified-linear-units-are-linear-how-do-they-add-nonlinearity-40247d3e4792) \n",
    "\n",
    "![Annulus problem](img/relu.png)\n",
    "\n",
    "\"Whereas tanh, a smooth, curved function, draws a clean envelope around the circle (and linear fails completely), ReLU draws a hexagon, with several pointed corners. In fact, this is what ReLU’s advantage is: it can bend the linear function at a certain point, to a certain degree. Combined with the biases and weights from the previous layer, the ReLU can take the form of a bend at any location at any degree.\n",
    "\n",
    "These small bends form the building blocks of approximations. Any relationship or function can be roughly estimated by aggregating many ReLU functions together, which occurs when neurons are collapsed and combined in the following layer. This has been mathematically proven, for example, with a sine wave or an exponential function, much like how a lower-degree Taylor series fits a function.\n",
    "\n",
    "The strength of the ReLU function lies not in itself, but in an entire army of ReLUs. This is why using a few ReLUs in a neural network does not yield satisfactory results; instead, there must be an abundance of ReLU activations to allow the network to construct an entire map of points. In multi-dimensional space, rectified linear units combine to form complex polyhedra along the class boundaries.\n",
    "\n",
    "Here lies the reason why ReLU works so well: when there are enough of them, they can approximate any function just as well as other activation functions like sigmoid or tanh, much like stacking hundreds of Legos, without the downsides. There are several issues with smooth-curve functions that do not occur with ReLU — one being that computing the derivative, or the rate of change, the driving force behind gradient descent, is much cheaper with ReLU than with any other smooth-curve function.\n",
    "\n",
    "Another is that sigmoid and other curves have an issue with the vanishing gradient problem; because the derivative of the sigmoid function gradually slopes off for larger absolute values of x.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d7de3e9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 32, 512])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.ones((32,32,512)).unsqueeze(0)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b12a03c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 512])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:, 0, :, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f46cb4da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['G:',\n",
       " 'My Drive',\n",
       " 'computer_vision',\n",
       " 'code',\n",
       " 'paper_reviews',\n",
       " 'adaptive_mixture_regression']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.normpath(os.getcwd()).split(os.sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7d1b8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa00725",
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_weights:\n",
    "    #self._initialize_weights()\n",
    "\n",
    "    mod = models.vgg16(pretrained=False)\n",
    "    pretrain_path = './models/Pretrain_model/vgg16-397923af.pth'\n",
    "    print(f'path: {os.getcwd()}')\n",
    "    mod.load_state_dict(torch.load(pretrain_path))\n",
    "\n",
    "    new_state_dict = OrderedDict()\n",
    "    for key, params in mod.features[0:23].state_dict().items():\n",
    "        new_state_dict[key] = params\n",
    "    self.layer3.load_state_dict(new_state_dict)\n",
    "\n",
    "    new_state_dict = OrderedDict()\n",
    "    for key, params in mod.features[23:30].state_dict().items():\n",
    "        key = str(int(key[:2]) - 23) + key[2:]\n",
    "        new_state_dict[key] = params\n",
    "    self.layer4.load_state_dict(new_state_dict)\n",
    "\n",
    "    new_state_dict = OrderedDict()\n",
    "    for key, params in mod.features[23:30].state_dict().items():\n",
    "        key = str(int(key[:2]) - 23) + key[2:]\n",
    "        new_state_dict[key] = params\n",
    "    self.layer5.load_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865e8ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16_LCM_REG(nn.Module):\n",
    "    def __init__(self, load_weights=False, stage_num=[3,3,3], count_range=patch_max, lambda_i=1., lambda_k=1.):\n",
    "        super(VGG16_LCM_REG, self).__init__()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.normal_(m.weight, std=0.01)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x3 = self.layer3(x)\n",
    "        x4 = self.layer4(x3)\n",
    "        x5 = self.layer5(x4)\n",
    "\n",
    "        if self.multi_fuse:\n",
    "            x5 = self.fuse_layer5(x5)\n",
    "            x4 = self.fuse_layer4(x4)\n",
    "            x3 = self.fuse_layer3(x3)\n",
    "\n",
    "        x5_= self.count_layer5(x5)\n",
    "        p5 = self.layer5_p(x5_)\n",
    "        if self.soft_interval:\n",
    "            k5 = self.layer5_k(x5_)\n",
    "            i5 = self.layer5_i(x5_)\n",
    "\n",
    "        x4_ = self.count_layer4(x4)\n",
    "        p4 = self.layer4_p(x4_)\n",
    "        if self.soft_interval:\n",
    "            k4 = self.layer4_k(x4_)\n",
    "            i4 = self.layer4_i(x4_)\n",
    "\n",
    "        x3_ = self.count_layer3(x3)\n",
    "        p3 = self.layer3_p(x3_)\n",
    "        if self.soft_interval:\n",
    "            k3 = self.layer3_k(x3_)\n",
    "            i3 = self.layer3_i(x3_)\n",
    "\n",
    "        stage1_regress = p5[:, 0, :, :] * 0\n",
    "        stage2_regress = p4[:, 0, :, :] * 0\n",
    "        stage3_regress = p3[:, 0, :, :] * 0\n",
    "\n",
    "        for index in range(self.stage_num[0]):\n",
    "            if self.soft_interval:\n",
    "                stage1_regress = stage1_regress + (float(index) + self.lambda_i * i5[:, index, :, :]) * p5[:, index, :, :]\n",
    "            else:\n",
    "                stage1_regress = stage1_regress + float(index) * p5[:, index, :, :]\n",
    "        stage1_regress = torch.unsqueeze(stage1_regress, 1)\n",
    "        if self.soft_interval:\n",
    "            stage1_regress = stage1_regress / ( float(self.stage_num[0]) * (1. + self.lambda_k * k5) )\n",
    "        else:\n",
    "            stage1_regress = stage1_regress / float(self.stage_num[0])\n",
    "\n",
    "\n",
    "        for index in range(self.stage_num[1]):\n",
    "            if self.soft_interval:\n",
    "                stage2_regress = stage2_regress + (float(index) + self.lambda_i * i4[:, index, :, :]) * p4[:, index, :, :]\n",
    "            else:\n",
    "                stage2_regress = stage2_regress + float(index) * p4[:, index, :, :]\n",
    "        stage2_regress = torch.unsqueeze(stage2_regress, 1)\n",
    "        if self.soft_interval:\n",
    "            stage2_regress = stage2_regress / ( (float(self.stage_num[0]) * (1. + self.lambda_k * k5)) *\n",
    "                                                (float(self.stage_num[1]) * (1. + self.lambda_k * k4)) )\n",
    "        else:\n",
    "            stage2_regress = stage2_regress / float( self.stage_num[0] * self.stage_num[1] )\n",
    "\n",
    "\n",
    "        for index in range(self.stage_num[2]):\n",
    "            if self.soft_interval:\n",
    "                stage3_regress = stage3_regress + (float(index) + self.lambda_i * i3[:, index, :, :]) * p3[:, index, :, :]\n",
    "            else:\n",
    "                stage3_regress = stage3_regress + float(index) * p3[:, index, :, :]\n",
    "        stage3_regress = torch.unsqueeze(stage3_regress, 1)\n",
    "        if self.soft_interval:\n",
    "            stage3_regress = stage3_regress / ( (float(self.stage_num[0]) * (1. + self.lambda_k * k5)) *\n",
    "                                                (float(self.stage_num[1]) * (1. + self.lambda_k * k4)) *\n",
    "                                                (float(self.stage_num[2]) * (1. + self.lambda_k * k3)) )\n",
    "        else:\n",
    "            stage3_regress = stage3_regress / float( self.stage_num[0] * self.stage_num[1] * self.stage_num[2] )\n",
    "\n",
    "        # regress_count = stage1_regress * self.count_range\n",
    "        # regress_count = (stage1_regress + stage2_regress) * self.count_range\n",
    "        regress_count = (stage1_regress + stage2_regress + stage3_regress) * self.count_range\n",
    "\n",
    "        return regress_count\n",
    "\n",
    "    def VGG_make_layers(self, cfg, in_channels=3, batch_norm=False, dilation=1):\n",
    "        d_rate = dilation\n",
    "        layers = []\n",
    "        for v in cfg:\n",
    "            if v == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=d_rate, dilation=d_rate)\n",
    "                if batch_norm:\n",
    "                    layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "                else:\n",
    "                    layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "                in_channels = v\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "class Count_layer(nn.Module):\n",
    "    \"\"\"\n",
    "    Avg pool, max pool, concat\n",
    "    1x1 conv/merge concatenated layer\n",
    "    \"\"\"\n",
    "    def __init__(self, inplanes=512, pool=2):\n",
    "        super(Count_layer, self).__init__()\n",
    "        self.avgpool_layer = nn.Sequential(\n",
    "            nn.Conv2d(inplanes, inplanes, kernel_size=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AvgPool2d((pool, pool), stride=pool),\n",
    "        )\n",
    "        self.maxpool_layer = nn.Sequential(\n",
    "            nn.Conv2d(inplanes, inplanes, kernel_size=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d((pool, pool), stride=pool),\n",
    "        )\n",
    "        self.conv1x1= nn.Sequential(\n",
    "            nn.Conv2d(inplanes*2, inplanes, kernel_size=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_avg = self.avgpool_layer(x)\n",
    "        x_max = self.maxpool_layer(x)\n",
    "\n",
    "        x = torch.cat([x_avg, x_max], dim=1)\n",
    "        x = self.conv1x1(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DC_layer(nn.Module):\n",
    "    \"\"\"\n",
    "    Direct counting layer\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, level, fuse=False):\n",
    "        super(DC_layer, self).__init__()\n",
    "        self.level = level\n",
    "        self.conv1x1_d1 = nn.Conv2d(512, 512, kernel_size=1)\n",
    "        self.conv1x1_d2 = nn.Conv2d(512, 512, kernel_size=1)\n",
    "        self.conv1x1_d3 = nn.Conv2d(512, 512, kernel_size=1)\n",
    "        self.conv1x1_d4 = nn.Conv2d(512, 512, kernel_size=1)\n",
    "\n",
    "        self.conv_d1 = nn.Conv2d(512, 512, kernel_size=3, padding=1, dilation=1)\n",
    "        self.conv_d2 = nn.Conv2d(512, 512, kernel_size=3, padding=2, dilation=2)\n",
    "        self.conv_d3 = nn.Conv2d(512, 512, kernel_size=3, padding=3, dilation=3)\n",
    "        self.conv_d4 = nn.Conv2d(512, 512, kernel_size=3, padding=4, dilation=4)\n",
    "        \n",
    "        self.fuse = fuse\n",
    "        if self.fuse:\n",
    "            self.fuse = nn.Conv2d(512*2, 512, kernel_size=3, padding=1)\n",
    "            self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.conv1x1_d1(x)\n",
    "        x2 = self.conv1x1_d2(x)\n",
    "        x3 = self.conv1x1_d3(x)\n",
    "        x4 = self.conv1x1_d4(x)\n",
    "\n",
    "        x1 = self.conv_d1(x1)\n",
    "        x2 = self.conv_d2(x2)\n",
    "        x3 = self.conv_d3(x3)\n",
    "        x4 = self.conv_d4(x4)\n",
    "\n",
    "        # x = torch.cat([x1, x2, x3, x4], dim=1)\n",
    "        # x = self.relu(self.fuse(x))\n",
    "        x = Maxout(x1, x2, x3, x4)\n",
    "        return x\n",
    "\n",
    "def Maxout(x1, x2, x3, x4):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        x1:\n",
    "        x2:\n",
    "        x3:\n",
    "        x4:\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # mask for x1 >= x2\n",
    "    mask_1 = torch.ge(x1, x2)\n",
    "    mask_1 = mask_1.float()\n",
    "    x = mask_1 * x1 + (1-mask_1) * x2\n",
    "\n",
    "    mask_2 = torch.ge(x, x3)\n",
    "    mask_2 = mask_2.float()\n",
    "    x = mask_2 * x + (1-mask_2) * x3\n",
    "\n",
    "    mask_3 = torch.ge(x, x4)\n",
    "    mask_3 = mask_3.float()\n",
    "    x = mask_3 * x + (1-mask_3) * x4\n",
    "    return x\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = VGG16_LCM_REG(load_weights=False)\n",
    "\n",
    "    model.eval()\n",
    "    image = torch.randn(2, 3, 384, 384)\n",
    "    x5, c = model(image)\n",
    "    # print(model)\n",
    "    print(\"input:\", image.shape)\n",
    "    # print(\"x5:\", x5.shape)\n",
    "    print(\"c:\", c.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
